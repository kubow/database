**QLIK for Data Lakes** - Streaming Data Pipeline Automation from Ingestion to Analytics

Continuously updated, analytics-ready data no-coding lakes **[Qlik Replicate](./Qlik.Replicate.md)™** and **[Qlik Compose](./Qlik.Compose.md)™ for Data Lakes**

Why to keep Data Lake:
- Economics - Large data at low cost
- New Data Types + Flexibility & Scale
- Advanced Analytics

**QDI for Data Lake Creation** helps enterprises deliver a return on their data lake investment by continuously providing accurate, timely and trusted transactional data sets for business analytics. Unlike other solutions, QDI for Data Lakes provides a multi-zone architecture with full historical and operational data store, supports virtually all industry standard data sources and targets, automates data flow from initial ingest to final consumption, profiles & catalogs the content of the data lake, and requires no hand coding or Hadoop scripting expertise. Data engineers can now meet growing demands for analytics-ready data sets in real time because QDI for Data.

## Real-time data ingestion and update

A simple and universal solution for continually ingesting enterprise data into popular data lakes in real-time.

## Automated and continuous refinement

A model-driven approach for quickly designing, building, and managing data lakes on-premises or in the cloud.

## Trusted, enterprise-ready data

Deliver a smart enterprise-scale data catalog to securely share all of your derived data sets with business users.

[Managed Data Lakes – Qlik Playbook](https://playbook.qlik.com/?page_id=5601)

Data Lake - a centralized repository for all data (un/structured)

- For data architects and engineers
- Rapidly deliver Real-time and Analytics-ready data sets from Data Lake
- Remove the time cost and risk of manual coding
- Adaptable to new sources, targets, platforms and technologies

Data Lake Creation Features:

- Automate the creation of tables, organize data structures and track lineage to deliver a managed data lake
- Instantiate data, maps source and target, and continously synchronize source and target schema
- Use Apache Spark and Apache Hive for refinement
- Replicate data in real time from source to target

Summary (3 points):

1. Real-Time Data for Faster Better Insights
2. Automated Continous Refinement
3. Trusted Enterprise-ready Data

Business Value

• Rescue failing “version 1 data lake” initiatives by producing tangible data lake ROI
• Deliver “analytics-ready certainty” so businesses can confidently identify and leverage data-driven opportunities
• Increase business agility and flexibility by deploying a solution that can adapt to both expected and unexpected data lake architectural changes

Solution Features

• End-to-end data pipeline automaton
• Multi-zone data lake methodology with full historical data store
• Metadata Integration with observable data lineage
• Continuously streaming data and meta-data updates via CDC
• Centralized enterprise-grade administration and management
• Universal connectivity – supports virtually all industry standard data sources and data lake targets, whether on-premises or in the cloud
• Intuitive visual interface that requires no hand coding or Hadoop scripting expertise
• A secure, enterprise-scale data marketplace that provides data consumers with a single, go-to catalog

Solution Benefits

• Immediate availability of transactional data for analytics
• Trusted data that provides analytics-ready certainty
• Improves operational efficiency with re-usable, automated pipelines
• Establishes best practices for keeping data in the lake fresh and up- to-date
• Future-proof flexibility that adapts to changing data lake technology
• Greater resiliency by rapidly propagating source or model changes through the data lake environment
• Understand the content and quality of your data warehouse for regulations like GDPR and CCPA